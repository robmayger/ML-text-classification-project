{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f470bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from cleaning.cleaners import BasicTextCleaner\n",
    "from tokenisation.tokenisers import BasicTokeniser\n",
    "from word_encoding.word_encoders import BasicEncoder\n",
    "\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589c5ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fetch_20newsgroups(subset='train')\n",
    "\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdff95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Get the category names\n",
    "label_names = data.target_names\n",
    "\n",
    "cleaner = BasicTextCleaner()\n",
    "X_clean = [cleaner.clean_text(text) for text in X]\n",
    "\n",
    "print(X_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cee1484",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokeniser = BasicTokeniser()\n",
    "X_tokens = [tokeniser.tokenise(text) for text in X_clean]\n",
    "\n",
    "print(X_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbd483b",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_doc_len = max(X_tokens, key=len)\n",
    "\n",
    "flat = list(chain.from_iterable(X_tokens))\n",
    "encoder = BasicEncoder(flat)\n",
    "X_encodings = [encoder.encode(doc, max_len=max_doc_len) for doc in X_tokens]\n",
    "print(X_encodings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
